# SSL-NBV: A self-supervised-learning-based next-best-view algorithm for efficient 3D plant reconstruction by a robot  

# SSL-NBV: 一种基于自监督学习的机器人高效3D植物重建的下一个最佳视角算法

Jianchao Ci \*, Eldert J. van Henten , Xin Wang , Akshay K. Burusa , Gert Kootstra  

瓦赫宁根大学与研究中心植物科学系农业生物系统工程组，荷兰瓦赫宁根6700AA，邮政信箱16号

# 文章信息  

关键词:  
下一个最佳视角规划  
3D重建  
自监督学习  
弱监督学习  
信息增益  
机器人技术  
主动感知

# 摘要  

由于植物复杂的形状导致许多遮挡，其3D重建具有挑战性。下一个最佳视角（NBV）方法通过迭代选择新视角来最大化信息增益（IG）来解决这个问题。基于深度学习的NBV（DL-NBV）方法相比经典的基于体素的NBV方法展示了更高的计算效率，但当前方法需要使用真实植物模型进行大量训练，这使得它们在实际植物应用中不切实际。此外，这些方法依赖于使用预先收集的数据进行离线训练，限制了在变化的农业环境中的适应性。本文提出了一种基于自监督学习的NBV方法（SSL-NBV），该方法使用深度神经网络预测候选视角的IG。该方法允许机器人在任务执行过程中通过将新的3D传感器数据与先前收集的数据进行比较，并采用弱监督学习和经验回放来进行高效的在线学习，从而收集自己的训练数据。

在仿真和真实环境中使用交叉验证进行了全面评估。结果表明，与非NBV方法相比，SSL-NBV需要更少的视角进行植物重建。它在0.0038秒内实现了IG预测，比基于体素的NBV快800多倍，在线学习迭代时间为0.099秒。与基线DL-NBV相比，SSL-NBV减少了超过90%的训练标注。此外，SSL-NBV能够通过在线微调适应新场景。使用真实植物的结果也表明，所提出的方法可以学习有效地规划3D植物重建的新视角。最重要的是，SSL-NBV自动化了整个网络训练，并使用持续在线学习，使其能够在变化的农业环境中运行。

# 1. 引言

温室生产是一种广泛使用的蔬菜种植方法，具有延长植物生产周期、提高质量和产量等显著优势，最终导致种植者利润增加。选择合适的品种（即表型分析）以及作物生产过程本身，包括收获和去叶等任务，都需要大量劳动力。这限制了大规模温室生产，因为经验丰富的劳动力稀缺且劳动力成本高。使用机器人自动化这些劳动密集型任务可以帮助减少对人类劳动力的依赖，并可能提高温室生产力和生产效率。

机器人表型分析和温室操作（如收获和去叶）本质上基于相同的关键功能，即感知。然而，在复杂的温室环境中实现准确的机器人感知具有挑战性，因为存在遮挡和变化。遮挡主要由叶片引起，阻碍了充分的信息收集，从而增加了机器人操作的不确定性。变化源于动态的生长环境（例如光照）和固有的植物特性（例如形状、大小和纹理），使得某些系统在特定条件下可能有效，但缺乏对变化的鲁棒性。本研究的目的是开发一个鲁棒的机器人感知系统，在存在遮挡的情况下捕获足够的植物信息。该系统专门针对植物重建任务在植物表型分析场景中开发和评估，但经过小的修改和微调后，有潜力用于其他任务，如机器人收获。

植物表型分析是一组测量植物性状（如叶角、节间长度和叶面积）的方法，随后将这些测量结果与植物基因型和生长环境联系起来，这是植物育种中高效选择目标品种的关键技术（Poland和Rife，2012）。与传统的植物性状人工评估相比，通过整合机器视觉技术和机器人技术，机器人表型分析方法受到了广泛关注，展示了在自动、准确、无损和高通量方面进行植物表型分析的潜力（Atefi等，2019；Hartmann等，2011；Polder和Hofstee，2014；van der Heijden等，2012）。二维（2D）机器人表型分析方法已被广泛研究（Jansen等，2009；Minervini等，2014；Tisné等，2013）。然而，这些方法的一个显著缺点是它们无法准确测量在三维（3D）中表达的植物性状，如植物体积。或者，三维（3D）方法在3D数据中进行测量，提供更全面和准确的植物性状信息（Boogaard等，2023；Shi等，2019）。3D表型分析方法的一个必要首步是生成植物的3D数字重建，可以表示为点云（Boogaard等，2022，2021）或网格（Thapa等，2018；Vázquez-Arellano等，2018）。然而，由于显著的植物遮挡，准确的3D植物重建极具挑战性。解决这个问题的常见方法是使用多个视角（Golbach等，2016；Lu等，2017；Shi等，2019），通过这种方法，在单一视角中被遮挡的信息变得可用。然而，由于这些方法采用被动范式，数据收集局限于预定义的视角和轨迹，它们存在数据缺失、冗余信息收集以及需要不断微调以适应新植物的问题。

最近，农业机器人中的下一个最佳视角（NBV）方法引起了广泛关注（Gibbs等，2020，2018；Zaenker等，2021a；Zapotezny-Anderson和Lehnert，2019）。NBV采用主动感知范式，主动重新定位和调整相机方向。下一个视角的选择依赖于分析迄今为止观察到的数据，考虑当前目标，旨在最大化信息增益（IG）。IG是一个量化通过选择新视角可实现的信息增加的指标。与被动感知方法相比，这种方法能够实现更高效和灵活的数据收集（Atefi等，2021）。准确和高效的IG估计对于NBV方法至关重要。传统的NBV方法需要维护一个表示空间的占用网格，其中包含体素和相应的占用概率，指示体素是空闲还是被占用，我们称之为Voxel-NBV。候选视角的IG估计是通过从该视角向占用网格投射射线并计算射线穿过的体素（包括空闲和被占用的体素）来执行的。这种方法已应用于各种农业应用（Burusa等，2022；Gibbs等，2020，2018；Zaenker等，2021b）。这种方法很有用，因为它明确地估计了IG，使规划过程可解释，并提供了关于被占用和空闲空间的信息。然而，由于需要投射数百万条射线并在体素级别检查每条射线的转换，这种方法在计算和内存方面都很密集。

基于深度学习的下一个最佳视角（DL-NBV）方法旨在解决这些缺点。这些方法使用神经网络直接根据迄今为止观察到的数据预测IG，消除了计算昂贵的射线投射和体素级操作的需要。例如，Mendoza等（2020）提出了NBV-Net，它使用3D卷积神经网络直接预测一组预定义候选者的IG，使用累积的占用网格作为输入。同样，Zeng等（2020）提出了一种基于点云的神经网络，称为PC-NBV，它通过将累积的点云作为输入来预测所有候选者的IG值。PC-NBV通过消除将点云转换为体积图的步骤，提供了更高的规划效率。虽然这两种方法在重建效率和简单几何形状的人造物体的鲁棒性方面表现出色，但它们在重建复杂植物结构方面的性能尚未得到评估。此外，这两种方法都需要单独的训练步骤和完整的物体模型进行真实IG估计，这给在动态农业环境中的使用带来了挑战。在本文中，我们将基于现有的PC-NBV方法开发一种具有在线自监督学习能力的DL-NBV方法，并在专注于植物表型分析的农业场景中评估其性能。

为植物重建训练DL-NBV网络面临重大挑战，因为难以获取大量IG标注的训练数据。手动计算真实IG不切实际，因为它是主观的，难以准确测量IG值。然而，使用机器人，可以通过让机器人探索环境来自动化这一过程。因此，机器人可以自主收集和标注IG数据，甚至在任务执行过程中进行训练，从而以终身和自监督的方式提高性能。这种学习方法被称为自监督学习（SSL）（Deng等，2020）。SSL的一个挑战是IG的自动标注。Mendoza等（2020）、Wang等（2019）和Zeng等（2020）通过比较完整模型来计算候选视角的真实IG。然而，这些方法需要事先了解物体形状，这使得它们在农业应用中不可行，因为植物模型无法提前获得。为了解决这个问题，本研究提出了一种改进的IG计算指标，基于Zeng等（2020）的方法，但仅根据机器人收集的数据实现真实IG计算，从而实现自监督学习。

另一个阻碍当前DL-NBV方法在农业中应用的问题是它们对离线训练的依赖，即首先收集大量训练数据，训练方法，然后在单独的步骤中执行。这限制了方法继续改进或适应新目标和环境的能力（Kahn等，2020）。相反，在线学习算法将这两个过程集成到一个连续的反馈循环中，允许网络在执行任务的同时学习和优化，促进对新目标和环境的适应。然而，在线学习方法通常面临样本效率低的问题，因为它们专注于最新数据并丢弃历史数据（Zhang和Sutton，2017）。为了应对在线学习的挑战，Mnih等（2013a）在强化学习（RL）领域采用了一种称为"经验回放"的离策略训练方法，展示了改进的样本效率和泛化能力（详见第2.1.1节第5步的解释）。这启发了我们的工作。此外，现有的DL-NBV方法通常使用强监督数据进行训练，其中每个训练输入都需要所有候选者的IG标注作为真实值，导致数据收集效率低下。为了解决这个问题，我们采用了弱监督学习技术，以减少所需的IG标注数量，促进高效的训练数据收集和在线学习。更多讨论见第2.1.4节。

本文的目标是提出并评估一种用于3D机器人植物重建任务的新型SSL-NBV方法。该方法利用深度神经网络进行信息增益（IG）预测，与基于体素的NBV方法相比，显著提高了视角规划效率。与最先进的基于学习的NBV方法（如PC-NBV）相比，该方法具有以下几个关键贡献：（1）集成了自监督学习（SSL），实现了在线网络训练，使模型能够在任务执行过程中适应新植物；（2）引入了一种IG度量方法，消除了在真实IG计算中需要完整植物模型的需求；（3）采用经验回放技术提高了样本效率；（4）采用弱监督学习提高了训练数据收集效率并减少了标注需求。该方法在仿真和真实场景中进行了测试，与各种基线规划器进行了全面评估。真实世界实验表明，该方法也能够学习真实植物重建的视角规划。本研究解决了以下问题：

1. 与其它NBV（PC-NBV和Voxel-NBV）和非NBV（随机和预定义）方法相比，所提出的SSL-NBV能实现怎样的重建质量和效率？

2. 使用在线和弱监督学习的SSL-NBV与使用离线和强监督学习的PC-NBV相比如何？

3. SSL-NBV能否在在线自监督学习过程中适应新环境？

4. 所提出的SSL-NBV方法能否应用于噪声较大的真实场景？

# 2. 方法与材料

本节描述了方法和实验设置。第2.1节描述了植物重建的在线自监督学习过程。第2.2节详细介绍了包括仿真和真实场景在内的实验设置。最后，第2.3节介绍了用于评估所提出方法性能的指标。

# 2.1 植物重建的在线自监督学习

整个学习过程涉及连续迭代。在每次迭代中，机器人使用深度神经网络进行下一个最佳视角规划，从一组候选视角中选择一个新的相机位姿，以收集3D植物重建的新数据，同时生成新的训练数据来更新网络。这种在线学习过程允许网络在任务执行过程中更新，而无需单独的训练阶段。这使得机器人能够持续改进并适应不断变化的环境。该算法首先在仿真中开发和验证，以便快速开发和重复实验，然后在真实世界应用中进行评估。

整个学习过程最多包含T次迭代。候选集$C = \{ c _ { 1 } , \cdots , c _ { M } \}$在整个学习过程之前定义，其中$\boldsymbol { c } _ { i } \in \mathbb { R } ^ { 6 }$是由位置和方向$\left\{ { x } _ { i } , { y } _ { i } , { z } _ { i } , { \alpha } _ { i } , { \beta } _ { i } , { \gamma } _ { i } \right\}$组成的相机位姿，$M$是候选视角的总数。简而言之，学习过程通过迭代地从$c$中选择视角作为植物重建的相机位姿，并收集数据来更新网络。视角变换和数据收集使用安装有深度相机作为末端执行器的机械臂进行。从每个视角收集部分点云，所有视角收集的点云融合为数字植物重建。

图1展示了一次迭代的概述。一次迭代$t$包含七个步骤：（1）使用DL-NBV网络预测IG，$\widehat { G } _ { t }$，输入为当前累积点云$P _ { t } ^ { a }$和记录先前访问视角的视角选择状态$V _ { t }$；（2）基于${ \widehat { G } } _ { t }$选择下一个视角$\nu _ { t + 1 }$；（3）将相机移动到$\nu _ { t + 1 }$并收集新的部分点云$P _ { t + 1 } ^ { c }$；（4）基于$P _ { t + 1 } ^ { c }$和$P _ { t } ^ { a }$计算实际IG $G _ { t }$；（5）存储以$P _ { t } ^ { a }$和$V _ { t }$为输入，$G _ { t }$为真实值的监督对；（6）更新累积点云$P _ { t + 1 } ^ { a }$和视角选择状态$V _ { t + 1 }$；（7）基于训练数据的一个批次训练DL-NBV的网络权重$\mathbb { W } _ { t + 1 }$。这些步骤将在下一小节中详细解释。

![](images/9f36919c12433ec16404c21863b0f11a7e9dc2298930c2fea537dc41333c639e.jpg)  
图1. 自监督学习中迭代步骤的示意图。在每次迭代步骤$t$中，进行下一个最佳视角规划以进行植物重建，并收集生成的数据，包括累积点云、视角选择状态和真实IG，以优化网络权重。蓝线表示训练样本的收集，虚线黑线表示数据更新，实线黑线表示操作后的数据转换。

# 2.1.1 迭代步骤详解

在步骤1中，当前累积点云$P _ { t } ^ { a }$和视角选择状态$V _ { t }$被输入到DL-NBV网络中，以预测$\widehat { \cal G } _ { t } = \{ \widehat { g } _ { 1 } , \cdots , \widehat { g } _ { M } \}$，其中包含所有候选视角的IG，${ \widehat { g } } _ { i } \in \mathbb { R }$表示一个视角的预测IG。$P _ { t } ^ { a } = \{ p _ { 1 } , \cdots , p _ { K } \}$是包含所有收集点的累积点云，其中$p _ { i } = \{ x _ { i } , y _ { i } , z _ { i } \} \in \mathbb { R } ^ { 3 }$是具有xyz坐标的3D点。$V _ { t } = \{ s _ { 1 } , \cdots , s _ { M } \}$是一个包含所有候选视角选择状态的向量，如果某个视角在过去被选择过，则$s _ { i } \in \{ 0 , 1 \}$等于1，否则为0。在学习过程开始时，$P ^ { a } { = } { \emptyset }$且$V = \{ 0 , \cdots , 0 \}$。在本研究中，我们使用PC-NBV网络（Zeng等，2020）作为DL-NBV规划器。第2.1.2节详细解释了PC-NBV网络结构和IG预测。

在步骤2中，基于$\widehat { G } _ { t ; { } }$，迭代t的估计下一个最佳视角确定为$\widehat { \nu } _ { t + 1 } = \mathop { \bf a r g m a x } _ { j = 1 } ^ { M } \widehat { g } _ { j }$，对应于获得最高预测IG的视角。然而，不是直接使用$\widehat { \nu } _ { t + 1 }$作为下一个相机视角$\nu _ { t + 1 }$，而是采用探索机制来保持在线学习中的探索-利用平衡。利用涉及选择$\widehat { \nu } _ { t + 1 }$作为下一个相机视角，这防止了浪费性的探索，而探索则涉及从候选集中随机选择一个视角$\nu ^ { r }$，以避免网络陷入次优的视角选择策略。我们设计了一种探索机制，遵循Wang等（2016）中常用的范式。该机制由探索率$\in _ { t } \in [ \in _ { m i n } , \in _ { i n i } ]$控制。在整个训练过程开始时，当$\ c =$ 1时，$\epsilon _ { t } = \epsilon _ { i n i }$被设置为初始探索率。然后，$\in _ { t }$每迭代一次以衰减率$\rho$衰减，直到达到最小值$\in _ { m i n . }$，根据$\in _ { t } = \operatorname* { m a x } \left( \in _ { m i n } , \rho ^ { t - 1 } \cdot \in _ { i n i } \right)$。在本工作中，我们使用$\rho = 0 . 9 5$作为常数。下一个视角的确定如下：

$$
\nu _ { t + 1 } = \left\{ \begin{array} { l } { { \widehat { \nu } _ { t + 1 } , 如果 { \in } _ { t } < x \sim U ( 0 , 1 ) } } \\ { { \nu ^ { r } \in _ { R } C , 否则 . } } \end{array} \right.
$$  

如果从0到1的均匀分布中随机生成的数$x$大于$\in _ { t } ,$ 则下一个视角$\nu _ { t + 1 }$设置为$\widehat { \nu } _ { t + 1 }$，否则从候选集$C$中随机选择。这种探索机制使系统能够在开始时进行广泛探索，并随着网络从经验中改进而逐渐转向使用自收集的信息。

在第3步中，确定下一个视角后，将相机位姿更改为该视角，从而可以收集新的局部点云$P _ { t + 1 } ^ { c }$。点云收集和处理的详细说明见第2.1.3节。

在第4步中，基于$P _ { t } ^ { a }$和$P _ { t + 1 } ^ { c }$，计算视角$\nu _ { t + 1 }$的真实IG $g _ { \nu _ { t + 1 } }$，然后将其转换为$G _ { t } = \{ 0 , \cdots , g _ { \nu _ { t + 1 } } , \cdots , 0 \}$作为网络训练的目标向量。为了在真实农业场景中实现持续在线学习，机器人应该能够从其自身收集的数据中计算$g _ { \nu _ { t + 1 } }$。为此，我们提出了一种方法，使真实IG的计算仅基于机器人从该视角收集的$P _ { t + 1 } ^ { c }$和之前收集的数据$P _ { \mathrm { \Delta } _ { t } } ^ { a }$。简而言之，$g _ { \nu _ { t + 1 } }$计算为$P _ { t + 1 } ^ { c }$中提供新信息的点的比例，这些信息在$P _ { t } ^ { a }$中未被捕获。如图2所示，$P _ { t + 1 } ^ { c }$中新观察到的点子集，记为$P _ { t + 1 } ^ { n }$（标记为橙色），通过移除与$P _ { t } ^ { a }$的交集（标记为绿色）获得，计算为$P _ { t + 1 } ^ { n } = P _ { t + 1 } ^ { c } - \left( P _ { t + 1 } ^ { c } \cap P _ { t } ^ { a } \right)$。然后，视角$\nu _ { t + 1 }$的真实IG计算为$g _ { \nu _ { t + 1 } } ~ = ~ \left| P _ { t + 1 } ^ { n } \right| / \left| P _ { t + 1 } ^ { c } \right|$，其中$| \bullet |$表示点集的大小。

![](images/d71930625d715d9ea6dd97a642736d82089cfea3711ae504437facf1ef7e8e00.jpg)  
图2. 不同类型点云的示意图。一个视角的真实IG等于该视角观察数据中新点的比例。$P ^ { a }$（蓝色和绿色点）指累积点云。$P ^ { c }$（橙色和绿色点）指新收集的局部点云。$P ^ { n }$（橙色点）指$P ^ { c }$中新收集的点。绿色点指$P ^ { a }$和$P ^ { c }$的交集。$P ^ { o }$（灰色点）指完整点云，用于评估阶段。

然而，交集$P _ { t + 1 } ^ { c } \cap P _ { t } ^ { a }$不能严格使用集合交集计算，因为$P _ { t + 1 } ^ { c }$和$P _ { t } ^ { a }$中的对应点不会具有完全相同的坐标。相反，我们通过计算$P _ { t } ^ { a }$中每个点与$P _ { t + 1 } ^ { c }$中每个点之间的欧几里得距离来确定$P _ { t + 1 } ^ { c } \cap P _ { t } ^ { a }$。如果最小距离低于阈值$\delta$，则该点被添加到交集中。因此，交集定义为：

$$
P _ { t + 1 } ^ { c } \cap P _ { t } ^ { a } = \left\{ p _ { k } | p _ { k } \in P _ { t + 1 } ^ { c } \land \underset { p _ { j } \in P _ { t } ^ { a } } { \operatorname* { m i n } } | | p _ { k } - p _ { j } | | _ { 2 } \leq \delta \right\}
$$  

其中$\| \bullet \| _ { 2 }$是两点之间的欧几里得距离。$\delta$在仿真中设置为$0 . 0 0 3 \mathrm { m }$，在真实场景中设置为$0 . 0 1 \mathrm { { m } }$。这与用于点云下采样的体素大小一致（见第2.1.3节）。该方法允许仅基于机器人收集的数据计算真实IG，在与机器人系统的灵活性结合时，实现了自主训练数据收集。

为网络训练制定的目标向量$G _ { t }$仅包含单个视角的稀疏真实IG，称为弱监督标注。使用弱监督数据进行网络训练的详细说明见第2.1.4节。

在第5步中，累积点云$P _ { t } ^ { a }$和视角选择状态$V _ { t }$被收集为输入，与目标向量$G _ { t }$一起制定训练样本$\boldsymbol { b _ { t } } = \left\{ P _ { t } ^ { a } , V _ { t } , G _ { t } \right\}$，然后存储在循环缓冲区$D = \{ b _ { t - l + 1 } , \cdots , b _ { t - 1 } , b _ { t } \} ,$其中$l \in \mathbb { N }$是可以存储的最大样本数。当$D$的容量达到时，最旧的数据被最新数据替换，保持固定大小的过去经验表示。$D$在每个植物重建周期结束时不会被清除，允许历史经验在未来的重建周期中重复使用。

在第6步中，更新累积点云和视角选择状态。通过添加新收集的点云来更新累积点云，$P _ { t + 1 } ^ { a } = P _ { t } ^ { a } \cup P _ { t + 1 } ^ { c } = P _ { t + 1 } ^ { c } - \left( P _ { t + 1 } ^ { c } \cap P _ { t } ^ { a } \right) \cup P _ { t } ^ { a }$（$P _ { t + 1 } ^ { c } \cap P _ { t } ^ { a }$的计算见式(2)），并通过将$s _ { \nu _ { t + 1 } }$设置为1来更新视角选择状态为$V _ { t + 1 }$。

最后，在第7步中，从$D$中随机采样一个子集$B _ { t } \subseteq _ { R } D$来优化网络的权重$\mathbb { W } _ { t }$，其中下标$R$表示训练样本的随机选择。$B _ { t } = \{ s _ { 1 } , \cdots , s _ { N } \}$包含$N$个训练样本，其中$N \in \mathbb { N }$是批量大小，当$D$中存储的样本数量超过批量大小时，网络训练开始。由于$D$在每个重建周期后不会被清除，因此$B _ { t }$包含来自当前和之前重建周期的数据。这种将历史数据收集在缓冲区中，然后从缓冲区中采样数据来训练网络的方法称为经验回放。经验回放是在线学习中的一项关键技术，通过重用历史数据来提高样本效率，并通过打破连续步骤之间的时间相关性来提高训练稳定性。

# 2.1.2 信息增益预测的深度学习网络结构

图3展示了本研究中使用的网络架构，该架构遵循了PC-NBV原始论文(Zeng等，2020)的设计。在任何迭代步骤t中，网络以$P _ { t } ^ { a }$和$V _ { t }$作为输入。首先，点云$P _ { t } ^ { a }$通过特征提取网络进行处理，提取局部特征以生成每点264维的点级特征$F O$。$F O$进一步通过最大池化处理，得到长度为264的全局特征$_ { G O }$。随后，G0和$V _ { t }$被复制以匹配$F O$的垂直维度。这种复制便于与$F O$进行拼接，从而生成点级特征$_ { F 1 }$，其中包含点云的全局和局部特征以及视角选择状态。$_ { F 1 }$然后输入到自注意力单元(Zhang等，2019)中，进一步整合这些特征，得到注意力特征$_ { F 2 }$。接着，应用一个具有2层、每层1024个神经元的多层感知器(MLP1)模块，经过最大池化后生成$_ { G 1 }$作为最终的全局特征。G1表示$P _ { t } ^ { a }$和$V _ { t }$，最后被输入到另一个具有4层、每层分别为1024、512、256和33个神经元的MLP2中。MLP2预测${ \widehat { G } } _ { t } = \{ { \widehat { g } } _ { 1 } , \cdots ,$ $\widehat { g } _ { M } \}$，包含所有候选视角的预测IG。

值得注意的是，原始的PC-NBV是为离线学习设计的，并在仿真环境中进行测试。我们修改了训练方法、IG计算指标(第2.1.1节第4步)和损失函数(第2.1.4节)，使其能够使用弱监督数据进行持续在线学习，展示了其在真实农业场景中的能力。

# 2.1.3 点云采集与处理

在仿真环境中，使用模拟的Intel Realsense L515 RGB-D相机从视角捕获颜色和深度信息，然后将其组合形成局部点云。随后使用VoxelGrid1滤波器进行点云下采样，体素大小为$0 . 0 0 3 \mathrm { m }$，生成$P ^ { c }$。体素大小的选择与Burusa等(2022)的工作一致，他们使用基于体素的NBV方法进行植物重建，这使得我们能够直接与他们的方法进行比较。

在真实世界实验中，使用Intel Realsense L515 RGB-D相机捕获点云。真实世界的点云通常包含各种噪声源，如环境光照波动、传感器特定伪影以及点云生成算法的特性。为了解决这个问题，我们实施了一个三步降噪过程。首先，使用RangeFilter消除落在指定范围之外的点。然后，通过应用统计离群值去除(SOR)滤波器对裁剪后的点云进行进一步细化，该滤波器识别并移除与点云平均值相比显著偏离其邻域的点。最后，应用体素大小为$0 . 0 1 \mathrm { { m } }$的VoxelGrid滤波器，进一步提高点云数据的清洁度。

# 2.1.4 使用弱监督数据的损失计算

在原始PC-NBV中，使用经典的均方误差(MSE)损失函数计算$\widehat { G } _ { t }$和$G _ { t }$之间的损失$L ^ { s }$(上标$s$表示强监督学习的损失)。$G _ { t }$被定义为$G _ { t } = \{ g _ { 1 } , \cdots , g _ { M } \}$，包含所有候选视角的真实IG。$L ^ { s }$计算如下：

$$
L ^ { s } = \sum _ { i = 1 } ^ { M } { ( g _ { i } - { \widehat { g } } _ { i } ) ^ { 2 } }
$$  

这种方法要求每个$\widehat { g } _ { i }$都与相应的$g _ { i }$相关联进行监督，换句话说，它需要完全的真实标签，使用强监督学习。虽然在仿真环境中有效，但这种方法对于真实世界场景中的机器人在线学习效率低下，因为机器人运动耗时，且获取$G _ { t }$需要机器人遍历$C$中的所有候选视角。

为了解决这个问题，我们提出了一种弱监督学习方法，使其能够基于粗粒度的真实标签进行训练，这是一种称为不精确监督的特定技术(Zhou, 2018)。我们将真实向量定义为$G _ { t } = \left\{ 0 , \cdots , g _ { \nu _ { t + 1 } } , \cdots , 0 \right\}$，其中$\nu _ { t + 1 }$是下一个视角。然后，弱监督损失$L ^ { w }$计算如下：

$$
L ^ { w } = \sum _ { i = 1 } ^ { M } a _ { i } ( g _ { i } - \widehat { \pmb { g } } _ { i } ) ^ { 2 }
$$  

其中

$$
a _ { i } = \left\{ { \begin{array} { l } { 1 , \ i f \ i = \nu _ { t + 1 } } \\ { 0 , \ o t h e r w i s e } \end{array} } \right.
$$  

这种方法允许仅基于下一个视角$\nu _ { t + 1 }$的真实值计算损失，显著提高了训练数据收集的效率。然而，这可能导致样本效率降低，因为只有与下一个视角相关的信息被用于损失计算和网络优化。为了提高样本效率，使用了经验回放(详情参见第2.1.1节第5步)。

# 2.2. 实验设置

我们设计了三个实验场景来评估SSL-NBV算法的性能：

(1) 模拟场景1（实验S1）：该场景评估网络在重建效率和质量、IG预测速度和训练效率方面的表现。我们将SSL-NBV算法与其他NBV和非NBV方法进行了比较（Q1）。
(2) 模拟场景2（实验S2）：在S1的基础上，该场景修改了视点设置和目标植物，以测试该方法对新视点设置和目标植物的泛化能力和适应性。在本实验中，我们比较了弱监督和强监督学习（Q2），并分析了在线微调后的性能改进（Q3）。使用在S1中训练的网络权重作为本实验微调的起点，以保留先前植物的知识。此外，该实验还检验了模型处理灾难性遗忘的能力——这是在线学习中常见的问题，即适应新数据会导致先前学习的信息丢失。为了测试这一点，将S2中微调后的网络重新应用于S1场景，以评估其保留早期知识的能力。
(3) 真实世界场景（实验RW）：该场景涉及在真实世界条件下使用配备RGB-D相机的机器人和真实植物测试算法（Q4）。在S1中训练的神经网络在本场景中持续进行微调。

![](images/87a2ad0d854b525de210c198c5ed66c1433a86b8d918af2e45f97ff3d73d7c33.jpg)  
图3. PC-NBV网络架构。网络首先提取累积点云的全局特征，然后将其与局部特征和视点选择状态结合，预测每个候选视点的IG。该图改编自Zeng等（2020）。

为了提高泛化能力，在训练过程中使用了多种植物。整个训练过程被分为多个植物重建周期。每个周期，一个具有随机姿态的植物被定位为目标，机器人从$c$中连续选择$n$个视点来重建植物，同时收集训练数据并更新网络。在$n$次迭代完成重建后，重新定位一个新植物，重新初始化累积点云和视点选择状态，并开始一个新的周期。通过初步测试，模拟实验设置为$n=10$，我们系统地增加了该值，10个视点通常允许SSL-NBV和基线方法进行良好的植物重建。这也足以在方法之间进行比较，而不会产生不必要的计算开销。对于真实世界实验，由于真实植物的复杂性更高，设置为$n=15$。在测试中使用相同的$n$值。虽然增加视点数量可能会略微提高所有方法的重建率，但总体性能趋势和方法之间的比较结果将保持不变。

# 2.2.1. 模拟场景

模拟使用Gazebo（Koenig和Howard，2004）开发，通过机器人操作系统（ROS）（Quigley等，2009）进行数据收集和交换。模拟在ThinkPad P15笔记本电脑上运行，配备Intel Xeon W-11855M CPU和Nvidia GeForce RTX A500 GPU（16 GB内存），操作系统为Ubuntu 20.04。

在实验S1和S2中，如图4所示，候选视点集$c$由33个视点组成$\left(M=33\right)$，围绕全局坐标系的原点呈圆柱形排列，从11个角度$(a_{1},\cdots,a_{11})$和3个高度$(h_{1},h_{2}$ $\begin{array}{r}{h_{3}。}\end{array}$)观察植物。这些视点都是水平的，并朝向原点的Z轴。相机可以在视点之间自由移动，提供植物的$360^{\circ}$视图。为了增加变化，每次创建植物时，从植物模型集中随机选择一个植物模型，并沿$\mathbf{x}$和y轴随机定位坐标$d_{x}$和$d_{y}$，以及旋转$\theta$。

在实验S1中，候选视点的半径$r$为$0.6\mathrm{m}$，不同高度为$h_{1}=0.04\mathrm{m}$，$h_{2}=0.25\mathrm{m}$，$h_{3}=0.46\mathrm{m}$。对于随机平移，$d_{x}$和$d_{y}$的选择范围设置为$d_{x},d_{y}\in U(-0.1,0.1)$，间隔为$0.02\mathrm{m}$，旋转设置为$\theta\in$ $U(0,360)$，间隔为$20^{\circ}$。使用了10个3D番茄植物模型2（T1-T10，如图5所示）作为目标，这些模型在结构、大小、高度、番茄穗数量和叶节点方面表现出差异。8个植物用于训练，2个用于测试。由于植物数量有限，采用K折交叉验证来评估该方法。这10个番茄植物被分为两类，A类（T1-T5）结构较简单，B类（T6-T10）结构较复杂。然后将这10个植物随机分为5组$(\mathrm{K}=5)$，每组包含来自每个类别的一个植物：组1（T3，T8），组2（T4，T6），组3（T5，T9），组4（T1，T7）和组5（T2，T10）。这种设置导致了五轮验证，每轮使用一组进行测试，其他组用于训练。每轮验证涉及对测试集进行50次植物重建，总共250次重复（5组$\times$50次重复）。每次重复从测试集中随机选择一个植物，将其放置在随机位置和方向，并让训练好的网络从随机视点开始重建植物。我们选择50次重复是为了确保重建过程中的足够变化，以反映方法的整体性能。

在实验S2中，修改了视点设置和目标植物，以创建一个新环境，测试所提出方法在完全新场景中的在线学习和微调能力。半径$(r)$设置为$0.5\mathrm{m}$，并使用更大的随机植物位置范围$d_{x},d_{y}\in U(-0.3,0.3)$，以引入与实验S1相比相机与植物之间相对姿态的更多变化。使用了三个辣椒植物3（P1-P3，如图5所示），这些植物在形态上与番茄植物显著不同。在评估过程中采用了K折交叉验证方法，$\mathrm{~K~}=3$。在每轮验证中，一个植物用于测试，而其余两个用于微调。每轮验证涉及50次重复，总共150次重复（3个植物$\times50$次重复）。

# 2.2.2. 真实世界场景

真实世界实验设置包括一台ABB IRB 1200机器人，其末端执行器上安装了一个Intel Realsense L515 RGB-D相机（参见图6a），便于从不同视角灵活收集数据。系统通过ROS进行机器人控制和设备间数据通信，使用与模拟相同的ThinkPad P15笔记本电脑。

由于机器人运动限制，我们在机器人坐标系x轴相对-60°到60°范围内以半圆柱形分布采样了$M=33$个视点（参见图6b）。圆柱扇区的半径为$0.45\mathrm{m}$，视点高度分别为$h_{1}=0.75\mathrm{m}$、$h_{2}=1.0\mathrm{m}$和$h_{3}=1.25\mathrm{m}$（相对于机器人坐标系原点）。我们选择了三株40天龄的番茄植株（RT1-RT3，如图5所示）作为目标。这些植株高度约为55cm，有6-7片复叶。真实植株的数据是预先收集的，这使我们能够重复实验并与基线方法进行比较。

J. Ci等

![](images/c05e552e4338c7bab77fdba3f2ca87dd038613184747fdd93d96a060083c04f5.jpg)  
图4. 视点采样和植株创建示意图。左图为俯视图，右图为侧视图。视点（蓝色箭头）以圆柱形分布采样，从11个角度$(a_{1},\cdots,a_{11})$和3个高度$(h_{1},h_{2},h_{3})$观察植株。每个植株在原点周围创建，在x轴和y轴上具有随机位置$d_{x}$和$d_{y}$，以及旋转$\theta$。

![](images/fef45fd13f2064333adfc08d0e9d85567882e129f3c46107fea8f0063aa0d6cb.jpg)  
图5. 实验中使用的植株。T1-T10是实验S1中使用的模拟番茄植株。P1-P3是实验S2中使用的模拟辣椒植株。RT1-RT3是实验RW中使用的真实番茄植株。

![](images/72633e5f290a876f91ac65c42807ee7384d651045d4bff7ab9f36ff98121ddfa.jpg)  
图6. 真实世界实验设置示意图。真实世界环境包括一台配备RGB-D相机的真实机器人，用于灵活的植株重建。

对于每株植物，其位置固定在机器人坐标系x、y、z轴上分别为$0.9\mathrm{m}$、0m和$0.65\mathrm{m}$。应用了$0^{\circ}$、$90^{\circ}$、$180^{\circ}$或$270^{\circ}$四种旋转，并收集了每种旋转下所有33个视点的点云。使用k折交叉验证方法测试该方法，每轮验证使用两株植物作为训练集，一株植物作为测试集，共进行三轮验证$(\mathbf{k}=3)$。每轮进行50次重复，总共进行150次植物重建重复（3株植物$\times50$次重复）。在每次重复中，选择设置为四种旋转之一的测试植物作为目标，并从随机选择的初始视点开始重建植物。

# 2.2.3. 实验中自监督学习的实现细节

在所有三个实验场景中，网络训练期间，经验回放的批量大小设置为$N=32$，缓冲区大小设置为$l=1000$。在将累积点云$P_{t}^{a}$输入网络之前，进行了下采样处理，将累积点云$P_{t}^{a}$调整为512个点，与植物大小无关，每个点从$P_{t}^{a}$中随机选择。该下采样程序的目的是在保持计算效率的同时，确保网络IG预测的点数一致。原始的$P_{t}^{a}$作为最终的3D重建持续维护和更新。根据粗略估计，模拟植物的最终3D重建$P_{t}^{a}$通常包含30,000到60,000个点，真实世界植物大约包含7,000到13,000个点，具体取决于目标植物的大小。选择512个点是基于初步测试，该测试表明使用更多点不会提高网络IG预测的准确性（或减少损失），并且由于网络架构复杂性的增加，会降低预测速度并线性增加计算和内存需求。

在实验S1中，探索参数设置为$\epsilon_{ini}=1.0$和$\in_{min}=0.2$，最大训练迭代次数设置为$T=50{,}000$。在实验S2中，由于网络已在实验S1中预训练，$\in_{ini}=0.2$降低到0.2，$T$设置为12,400，以保持训练集中每株植物的迭代次数相同（番茄8株，辣椒2株）。在S1期间维护和更新的回放缓冲区在S2在线学习开始前被清除，以便在这个新环境中进行有效的微调。在实验RW中，由于模拟和真实世界条件之间的显著差异，$\in_{ini}$重新设置为1.0以允许更多探索。考虑到真实世界植物的复杂性增加，最大迭代次数设置为$T=25,000$，高于实验S2。在线学习开始前清除了回放缓冲区，以便在RW中进行有效的微调。

# 2.2.4. 实验中基线规划器的实现 我们将SSL-NBV的性能与其他方法进行了比较：

(1) 传统基于体素的NBV（Burusa等，2022）：该方法将点云转换为占据网格，并使用从每个候选视点进行光线投射来计算下一个最佳视点的IG。为了加快视点规划速度，占据网格更新和光线投射过程使用GPU加速执行。光线投射、IG计算和占据计算的参数遵循原始论文。
(2) PC-NBV（Zeng等，2020）：该方法在批量大小、训练迭代次数和epoch方面与我们的SSL-NBV具有相似的训练设置。然而，PC-NBV依赖于强监督数据，需要通过遍历所有候选视点来收集真实IG，这个过程非常耗时，并且依赖于完整的植物模型，这在真实世界场景中是不可用的。

(3) 随机规划器：在该方法中，每个下一个视点从候选集$C$中随机选择。

(4) 预定义规划器：该规划器从候选集$c$中选择11个视点，以之字形分布在3个高度和11个角度上，每个角度交替高度。视点集表示为$\{a_{1}h_{1},a_{2}h_{2},a_{3}h_{3},a_{4}h_{2},a_{5}h_{1},a_{6}h_{2},a_{7}h_{3},a_{8}h_{2},a_{9}h_{1},a_{10}h_{2},a_{11}h_{3}\}$（参见图4）。在每次植物重建周期中，从该集合中随机选择10个不重复的视点，并以随机顺序访问，以消除视点顺序对植物重建效率的影响。

# 2.3. 评估指标

采用两个主要指标来评估所提出的方法：(a) 重建质量和效率；(b) 网络训练所需的地面真值标注数量。

# 2.3.1. 重建质量和效率

重建质量通过地面真值点云$P^{o}$的重建比例$(R)$来评估。在模拟环境中，植物模型的地面真值点云是通过在其网格表面均匀采样点生成的。这些点随后使用VoxelGrid滤波器进行下采样，空间分辨率为$0.003\mathrm{m}$。该过程生成的番茄植物点云包含约30,000到40,000个点，辣椒植物包含约50,000到60,000个点。在真实世界场景中，由于无法获得完整的3D模型，地面真值模型通过合并所有视点收集的点云来近似。随后应用第2.1.3节中描述的相同噪声过滤过程，并将地面真值点云下采样到$0.01\mathrm{m}$的分辨率。该过程生成的真实世界植物点云包含约7,000到9,000个点。虽然使用这种方法生成的地面真值并不完全准确，但它提供了一个合理的近似，因为所有规划器都在一致的标准下实现和评估。

当前重建轮次的重建比例计算为$P^{o}$中在当前累积点云中重建的点的比例，表示为$R = | P^{o} \cap P^{a} | / | P^{o} |$，其中交集$P^{a} \cap P^{o}$使用公式(2)计算。在实验S1和S2中，确定$P^{o}$中的点是否重建的阈值$\delta$设置为$0.003\mathrm{m}$，而在RW中，由于额外的传感器噪声，阈值设置为$0.01\mathrm{m}$。重建效率通过计算在植物重建周期中达到特定阈值$\tau = 0.8$和0.9所需的视点数量来评估。

IG预测速度会显著影响重建效率。更快的IG预测方法可以更快地选择下一个最佳视点，从而加快植物重建。IG预测速度通过预测所有33个候选视点的IG所需的时间来衡量，包括使用和不使用GPU加速的情况。此外，基于学习的NBV方法需要额外的时间进行数据收集和网络训练，这会影响整体植物重建效率。假设需要重建A株植物，计算了所提出方法（包括网络训练和植物重建）所需的总时间，并与基线方法进行了比较。

我们的SSL-NBV算法的性能通过K折交叉验证与基线规划器进行了比较。在每轮验证中，所有规划器都应用于相同的测试植物集，并使用相同的重复次数进行植物重建（详见第2.2.1和2.2.2节）。每种方法的结果通过平均这些K折交叉验证重复的结果获得。

# 2.3.2. 地面真值标注数量

每个地面真值IG的测量需要机器人将其末端执行器移动到候选视点，收集部分点云，并根据累积点云计算IG。因此，网络训练所需的地面真值标注数量显著影响在线学习方法的适应性。需要较少标注的方法可以减少机器人运动，加快数据收集，并促进更频繁的网络更新。

SSL-NBV所需的标注数量记为$A^{ssl} \in \mathbb{N}$，等于机器人运动的总次数，因为它使用在线弱监督学习，每次机器人运动都可以收集一个训练样本及其对应的地面真值IG。为了直接了解结果，将$A^{ssl}$的值与使用离线强监督学习的PC-NBV进行了比较。PC-NBV所需的标注数量计算为$A^{off} = F \times M$，其中$F$表示离线训练样本的总数，$M$表示候选视点的数量。

由于PC-NBV的训练数据是预先收集的，$A^{off}$在整个训练过程中保持不变。我们将$A^{off}$作为SSL-NBV在线训练期间允许的最大标注数量。随着训练的进行，SSL-NBV在植物重建中的性能提高，$A^{ssl}$增加直到达到$A^{off}$。实际上，当SSL-NBV的植物重建性能收敛到最大水平时，可以终止其训练，因为网络的权重已经稳定和优化。然而，确定确切的收敛点可能是主观的。为了解决这个问题，我们提出了一种统计方法，当SSL-NBV和PC-NBV的重建比例之间没有显著差异（$\mathbf{p}$值$> 0.05$）时，认为训练完成。然后使用该点计算SSL-NBV的$A^{ssl}$。使用双样本t检验计算两组结果之间的p值，假设两组结果都服从正态分布。

在实验S1中，收集了8,000个样本用于PC-NBV训练，得到$A^{off} = 264,000$（8000个样本$\times 33$个视点）。在实验RW中，收集了1,920个样本用于微调PC-NBV，得到$A^{off} = 63,360$（1920个样本$\times 33$个视点）。

# 3. 结果

# 3.1. 模拟场景1

实验S1的结果通过比较SSL-NBV与基线方法的植物重建效率和质量，回答了问题1。图7展示了SSL-NBV在交叉验证中与其他方法的平均重建率对比（可视化示例见图14）。所有NBV方法都优于非NBV方法，证实了NBV方法在植物重建中更为有效。我们的方法实现了约0.95的最终重建率，对应最终3D重建中的30,000到60,000个点，具体取决于目标植物的大小和结构复杂性。阈值$\tau = 0.8$和0.9分别在5个和6个视点后达到。这比非NBV方法更快，其中预定义规划器需要5个和7个视点，随机规划器需要7个和10个视点，表明SSL-NBV能够以更少的视点更高效地重建植物。与使用强监督学习训练的PC-NBV相比，SSL-NBV的重建效率略低，最终重建率降低了0.02。然而，SSL-NBV使用弱监督学习，仅需要稀疏的IG标签，与PC-NBV相比可以显著减少训练期间对真实IG标注的需求（后续将提供进一步分析）。所有规划器都未能实现100%的植物重建，因为视点仅限于水平视角，无法看到仅从其他角度（如向上或向下）可见的植物部分。

![](images/d708f1d6d81957bb5886b2da37aa81180a7a1128c39b78f1884dd52bd7bf2316.jpg)  
图7. 实验S1中SSL-NBV在交叉验证（250次植物重建周期）中与基线方法的平均重建率对比。

图8展示了SSL-NBV的植物重建率与在线学习期间使用的标注数量的关系。这是以T1-T7作为测试植物的代表性曲线，在其他验证轮次中也观察到了类似的模式。该图比较了弱监督（SSL-NBV）和强监督（PC-NBV）学习的样本效率，部分回答了问题2。与训练期间使用264,000个标注的PC-NBV相比，在此比较中，SSL-NBV的训练也扩展到了264,000次训练迭代，但图表显示，SSL-NBV在更少的标注下就达到了相似的性能。结果表明，SSL-NBV仅使用9%的标注就达到了与PC-NBV相似的植物重建率（p值$> 0.05$），表明其样本效率显著更高。这种提高的样本效率可归因于不同的训练方法。PC-NBV使用离线强监督学习，同时更新网络以提高所有视图的IG预测准确性。虽然这种方法可以增强泛化能力并减少训练中的波动，但也可能引入冗余或信息量较少的数据，降低样本效率。相反，SSL-NBV采用弱监督学习，在探索和利用之间取得平衡，这可能导致选择信息量更大的视图。

![](images/829a78bebb6dd8a7e2cfff9193dae5fb07dc09c42f26e85d2cc61b9fc27ced96.jpg)  
图8. 使用T1-T7作为测试植物，植物重建率与在线学习期间使用的训练标注数量的关系。$\mathbf{x}$轴显示了SSL-NBV使用的IG标注数量与PC-NBV使用的2.64$\times10^{5}$个标注的比率。红线表示SSL-NBV方法的趋势，蓝线作为参考绘制了PC-NBV方法的趋势。阴影区域表示50次植物重建周期的95%置信区间。十字${ \bf \Pi } ( { \bf x } )$表示SSL-NBV和PC-NBV之间重建率无显著差异（p值$> 0.05$）。垂直黑色虚线表示两种方法在重建率上首次无显著差异的IG标注比率。

表1 基于学习的NBV方法与使用光线投射的体素方法之间的IG预测速度比较。


| GPU/CPU | 规划器 | NBV规划时间(s) (平均值±标准差) | 带在线学习的NBV规划时间(s) (平均值±标准差) |
|---------|--------|-------------------------------|------------------------------------------|
| GPU     | SSL-NBV | 0.0038±0.00029                | 0.099±0.00024                            |
|         | PC-NBV  | 0.0037±0.00042                | -                                        |
| CPU     | Voxel-NBV | 3.2±0.0038                   | -                                        |
|         | SSL-NBV | 0.039±0.0037                  | 3.7±1.2                                  |
|         | PC-NBV  | 0.040±0.0038                  | -                                        |
|         | Voxel-NBV | 22±1.8                      | -                                        |

虽然当$A^{ssl} = A^{off}$时，SSL-NBV的平均重建率略低于PC-NBV，但差异在统计学上并不显著，这表明在相同数量的标注下，SSL-NBV可以实现与PC-NBV相似的植物重建性能。考虑到SSL-NBV在样本效率和在线学习适应性方面的显著优势，它非常适合自动植物重建任务。

此外，我们比较了SSL-NBV、PC-NBV和Voxel-NBV的IG预测速度，如表1所示。该速度通过每种方法预测所有候选视点IG所需的总时间来衡量。由于SSL-NBV和PC-NBV使用相同的网络架构，它们实现了相似的预测速度。当使用GPU时，我们的SSL-NBV仅需0.0038秒进行IG预测，比Voxel-NBV（3.2秒）提高了842倍。这突显了基于神经网络的NBV方法与使用光线投射进行IG预测的基于体素的方法相比的效率提升。当仅使用CPU时，我们的方法需要$0.039s$进行IG预测，比Voxel-NBV（22秒）提高了564倍。重要的是，这些时间测量仅考虑了IG预测。对于Voxel-NBV，由于需要额外的时间将点云数据转换为体素，两种方法之间的速度差异将进一步增大。

此外，SSL-NBV涉及额外的步骤，如地面真值IG计算、训练数据存储和NBV规划期间的在线网络训练，以针对新植物进行微调。在GPU加速下，一次带在线学习的NBV规划需要$0.099s$，比Voxel-NBV快32倍。在CPU上，由于地面真值IG计算和网络训练的计算，时间增加到3.7秒，但仍比Voxel-NBV快6倍。值得注意的是，在线网络训练仅在方法应用于新植物时才需要。对于已经训练过的植物，可以停用在线训练。

根据表1和图8中的数据，我们估算了不同方法部署3D植物重建所需的时间。在我们的模拟中，机器人在两个视点之间转换平均需要$0.47s$。对于仅消耗时间在植物重建上的Voxel-NBV，重建$A$株植物的总时间计算为$(3.2s + 0.47s) \times 10 \times A$，其中10表示每株植物所需的视点数量。对于PC-NBV，需要额外的时间进行数据收集和网络训练。总时间计算为264,0$0 \times ~ 0.47s ~ + ~ 1800s ~ + ~ (0.0037s ~ + ~ 0.47s) ~ \times ~ 10 ~ \times ~ A$，其中264,000对应于训练PC-NBV特定植物类型所需的最小标注数量（与图8一致），1800s表示网络训练过程。对于采用在线学习的SSL-NBV，其标注需求相比PC-NBV显著减少（见图8）。其总计算时间计算为$264,000 \times 0.09 \times (0.47\mathrm{{s}} + 0.099\mathrm{{s}}) + (0.0038\mathrm{{s}} +$ $0.47s) \times 10 \times A$，其中0.09表示SSL-NBV相比PC-NBV达到相似性能所需的标注比例。

图9展示了三种方法随着植物数量增加的时间消耗趋势。SSL-NBV始终比PC-NBV少需要约$35.84\mathrm{h}$，这归因于其减少的标注需求，从而减少了机器人运动。当重建少于422株植物时，Voxel-NBV比SSL-NBV更高效，因为它不需要训练。然而，对于更大规模的重建，SSL-NBV的效率变得越来越明显，与Voxel-NBV相比，整体时间消耗显著减少。值得注意的是，温室中的植物通常每周都会发生显著变化，这意味着不同生长阶段的植物可以被视为新植物，这使得超过422株植物的阈值并不困难。

# 3.2. 模拟场景2

实验S2的结果通过比较弱监督学习和强监督学习的泛化能力，回答了问题2，并通过测试所提出方法在新环境中的适应性，回答了问题3。图10展示了SSL-NBV在交叉验证中与其他NBV方法的平均重建率对比（可视化示例见图14）。由于与non-NBV方法的比较在实验S1中已清楚展示，且与问题2和3无关，这些方法被排除在图10之外。在本实验中，在实验S1中对番茄植物进行$T = 50{,}000$次迭代训练的SSL-NBV网络被持续微调用于辣椒植物。微调后的SSL-NBV展示了比未微调的SSL-NBV网络更高的重建效率和质量，分别以少1个和2个视点达到$\tau = 0.8$和0.9，展示了其自动优化新场景的适应性，回答了问题3。微调后的网络以比PC-NBV少1个视点达到$\tau = 0.8$。然而，两种方法在最终实现了相当的重建（最终3D重建中包含约50,000到60,000个点，具体取决于植物的大小），表明PC-NBV在这个新环境中具有高水平的泛化能力，并且可以在足够的视点下实现良好的植物重建。

![](images/0c6fd7e8a3a766246d53d8641f149118b9450570536977387ff7faa67aba89ef.jpg)  
图9. 各种方法部署在3D植物重建场景中所需的大致时间。X轴表示重建植物的数量，$\mathbf{y}$轴表示所需时间。

![](images/85dfc7b4c5f8ce081cd6c6d83c43af3bc4749360e17da6257665f761a2084360.jpg)  
图10. 实验S2中SSL-NBV在交叉验证（150次植物重建周期）中与基线方法的平均重建率对比。

未微调的SSL-NBV展示了比PC-NBV更低的重建效率，表明弱监督学习相比强监督学习提供了更低的泛化能力。然而，SSL-NBV以显著更少的标注（SSL-NBV为50,000，PC-NBV为264,000）进行训练，仍然实现了与PC-NBV相似的最终重建率（回答了问题2）。在自动在线微调后，SSL-NBV优于PC-NBV。尽管改进很小，我们将其归因于实验S2与实验S1仍有一些相似之处，使得PC-NBV能够泛化到这个环境。Voxel-NBV产生了最差的结果，可能是因为在本场景中扩大了网格空间以允许更多的植物位置变化（参见第2.2.1节）。这引入了更多的空体素，影响了Voxel-NBV的性能，因为该方法旨在重建整个网格空间，而不仅仅是植物本身。

此外，我们评估了模型处理灾难性遗忘的能力。该评估包括三个阶段：(1) 在番茄植物上的初始训练（与实验S1一致），(2) 在辣椒植物上的微调（与实验S2一致），以及(3) 使用阶段(2)后获得的网络权重在番茄植物上的重新训练。如图11所示，在阶段(3)开始时观察到重建率略有下降，表明对新植物的长期适应对SSL-NBV完全保留先前学习植物的知识提出了挑战。然而，该方法仍然实现了超过$90\%$的重建率，展示了其在番茄植物重建任务中的可行性。但对于需要更高重建精度的场景，可能需要进行网络重新训练以确保番茄植物的最佳结果。

# 3.3. 真实场景

实验RW的结果通过评估SSL-NBV方法在真实场景中的表现，回答了问题4。图12展示了SSL-NBV与其他方法在交叉验证中的平均重建率对比。SSL-NBV在这个真实场景中表现出色，在重建质量和效率上都优于非NBV规划器。具体而言，SSL-NBV在7个视点内达到了$\tau = 0.8$，分别比随机规划器和预定义规划器快了2个和3个视点。SSL-NBV在12个视点后达到了$\tau = 0.9$，这是非NBV方法未能达到的阈值。SSL-NBV重建了大约$92.5\%$的植物（对应3D重建中的7,000到13,000个点，取决于植物的大小），分别比随机规划器和预定义规划器提高了$3.5\%$和$8.5\%$。

值得注意的是，在本实验中，我们通过合并所有视点的点云来近似真实植物模型，这使得PC-NBV的真实IG计算和离线微调成为可能。然而，在实际应用中，真实植物模型不可用于PC-NBV的微调。相比之下，我们的SSL-NBV不需要真实植物模型，并自动化了整个训练/微调过程，以自监督的方式收集自己的训练数据，并持续优化网络以适应新的植物和环境。

图13展示了以RT3为测试植物时，SSL-NBV的重建率随在线学习期间使用的标注数量增加的趋势。在微调过程中，重新收集了1,920个样本来重新训练PC-NBV，产生了$A^{off} = 63,360$个IG标注。为了与PC-NBV（使用63,360个标注）对齐，SSL-NBV的训练扩展到了63,360次训练迭代。该图显示，SSL-NBV仅使用$21\%$的标注就达到了与PC-NBV相似的重建率（$\mathbf{\dot{p}}$值$> 0.05$），表明对真实标注的需求减少了$79\%$。

![](images/22c9cf1aeb89b74e47f11cbb35d71fb9961c26c8b58048ed06d58312c8ccdd89.jpg)  
图11. SSL-NBV在三个阶段在线训练期间的重建率：番茄植物的初始训练（红色）、辣椒植物的微调（蓝色）和番茄植物的重新训练（绿色）。

![](images/88ee6821e76129434ceb62aae958b683ad0fc6ab7d82bafec9466f028b17d35b.jpg)  
图12. 实验RW中SSL-NBV与基线方法的平均重建率对比，使用交叉验证评估（150次植物重建周期）。

![](images/bc816371694ed6f71e02fc1f596bfb6b535c27ce29a11e1d00637c73eb52e49e.jpg)  
图13. 以RT3为测试植物，植物重建率与在线学习期间使用的训练标注数量的关系。阴影区域表示50次植物重建周期的$95\%$置信区间。$\mathbf{x}$轴显示了SSL-NBV使用的IG标注数量与PC-NBV的比率。十字${ \bf \Pi } ( { \bf x } )$表示SSL-NBV和PC-NBV之间的重建率无显著差异（p值$> 0.05$）。

图14展示了各种植物类型的3D重建结果示例，包括模拟番茄植物（实验S1）、模拟辣椒植物（实验S2）和真实世界番茄植物（实验RW）。模拟和真实世界植物的点云在点密度上存在差异，这是由于使用了不同的体素大小进行点云下采样（详见第2.1.3节）。所有植物在重建周期结束时都达到了超过$90\%$的重建率，其中模拟植物允许10个视点，真实世界植物允许15个视点，证明了SSL-NBV在模拟和真实环境中的有效性。值得注意的是，真实世界植物的真实点云包含噪声，特别是在边缘处，这可能会降低重建率，因为噪声难以准确重建。

![](images/93e0a5a6d036eb0b8c5f799850effbbb05f378da8e3ca04c4c5e756d4a641d1b.jpg)  
图14. 实验S1（10个视点）、S2（10个视点）和RW（15个视点）的3D重建结果。真实点云显示为灰色，重建点云显示为绿色。每个重建的重建率显示在底部。模拟点云使用$0.003\mathrm{m}$的体素大小进行下采样，而真实世界点云使用$0.01\mathrm{m}$的体素大小，导致点密度存在差异。

# 4. 讨论

# 4.1. 与相关研究的比较

结果表明，基于学习的NBV方法实现了与经典基于体素的NBV方法相当的植物重建性能，同时显著提高了IG预测速度。这与Han等人（2022）的研究一致，他们开发了用于3D物体重建的双分支NBV网络（DB-NBV）并在合成数据集上进行了评估。我们的发现也支持Zapotezny-Anderson和Lehnert（2019）的研究，他们表明使用深度神经网络进行直接IG预测可以提高视点规划效率。尽管SSL-NBV在NBV规划期间需要在线训练，略微降低了整体效率，但它仍然比基于体素的方法更高效。重要的是，在线学习仅对新植物是必要的；对于之前微调过的目标，这一步骤是不必要的，从而保持了高效率。

与依赖完全标注IG数据的强监督学习的PC-NBV相比，我们的方法仅需要稀疏的IG标签，在显著减少真实IG标注数量的同时实现了相似的植物重建效率。我们的发现与Stutz和Geiger（2020）的研究一致，他们证明了在3D形状补全任务中使用$3\mathrm{-}10\%$的标注可以获得与完全标注训练数据相当的结果。同样，Cheng等人（2023）表明，在图像分割任务中，使用弱监督标签可以实现与完全标注数据相当的性能。据我们所知，将弱监督学习应用于NBV系统尚未被探索，现有的方法如Han等人（2022）、Zeng等人（2020）和Mendoza等人（2020）都依赖于大量的IG标签。

值得注意的是，我们的方法能够自动微调以适应新的植物和环境，无需人工干预。当与机器人技术集成时，它实现了终身和持续的在线学习，这对于植物和环境不断变化的机器人农业操作至关重要。这种能力在现有的基于学习的NBV方法中是不存在的，这些方法通常依赖于预先收集的离线数据进行训练。

我们对方法在真实世界植物重建中的性能进行了定量分析，这是类似研究中缺乏的重要评估。虽然Han等人（2022）将他们的方法应用于真实世界植物重建，但他们的评估是定性的，缺乏定量比较。此外，他们的方法需要完整的物体模型来进行真实IG计算，限制了其在真实世界植物中微调的能力，因为这些模型不可用。这个问题也存在于Zeng等人（2020）和Mendoza等人（2020）的工作中。相反，我们的方法通过改进IG指标并消除IG计算中对完整植物模型的需求，实现了对真实世界植物的持续微调。

总之，本研究提出了SSL-NBV，一种能够自动适应新植物和环境的方法，允许机器人在任务执行期间无需人工干预即可实现终身和持续的改进。通过在模拟和真实世界实验中的全面评估，我们展示了其在IG预测中的效率以及显著减少IG标注需求的能力。这项工作为未来NBV方法在农业中的应用提供了宝贵的见解，特别是在减少对大量标注的依赖和增强动态环境中的可扩展性方面。

# 4.2. 局限性与未来改进

# 4.2.1. 针对不同尺寸植物的灵活视点采样

我们的方法采用了全局视点采样方法，即预先采样一组候选视点，NBV规划器从中迭代选择以重建植物。这种方法效率较高，因为它能够全局搜索下一个最佳视点。然而，它将选择限制在固定视点，可能会错过最优视角，降低重建质量。此外，使用固定的候选视点集可能会限制该方法对不同尺寸植物的可扩展性。Zeng等人(2022)和Lehnert等人(2018)通过相对视点规划解决了这一挑战，即根据当前视点的增量运动确定下一个相机位姿。这些方法不受固定视点的限制，能够适应各种植物尺寸。然而，局部视点规划方法经常面临局部最优问题。在未来的工作中，我们计划将SSL-NBV与局部NBV方法相结合，既支持大规模搜索(如整株植物或植物行)的全局规划，又能对固定视点不可见的植物部位进行详细重建。

# 4.2.2. 相机视点轨迹优化

我们的方法以视点为单位进行NBV规划，仅基于信息增益(IG)最大化选择下一个最佳视点。然而，这可能导致视点轨迹效率低下，需要大量相机移动才能达到相似的植物重建水平。理想情况下，NBV规划应考虑视点轨迹优化，即在保持高重建效率的同时最小化相机移动。为了解决这个问题，我们建议使用深度强化学习(DRL)算法将轨迹优化机制集成到现有的NBV规划过程中。DRL广泛应用于路径规划，因为它考虑长期影响，学习最大化累积奖励的任务执行策略。在NBV问题中，将重建效率和相机移动都纳入奖励函数，可以在减少相机移动的同时实现高效的植物重建。另一个潜在的解决方案是在当前视点选择过程中添加约束，在权衡相机移动和预测IG后，只允许选择附近的视点。

# 4.3. 实验条件的影响

# 4.3.1. 候选视点集的大小和分布

候选集的大小会影响网络的训练效率和植物重建质量。增加候选集大小可以通过捕捉之前未见的植物部位来提高重建效果，但也会降低训练效率并增加对训练数据的需求。在本研究中，我们使用了33个视点的集合，在植物重建和训练效率之间取得了平衡。然而，对于较大的植物或植物行重建任务，可能需要扩展候选集以完全捕捉所有植物部位。更大的候选集可以放大NBV方法在植物重建中相对于非NBV方法的优势，因为非NBV方法更难选择高IG视点，而NBV方法在充分训练(对于DL-NBV)或时间(对于Voxel-NBV)的情况下，可以有效地识别下一个最佳视点，从而提高植物重建效果。

在本研究中，我们分别在模拟和真实环境中采用了圆柱形和半圆柱形分布，类似于Golbach等人(2016)的做法。这种配置有利于植物重建，因为植物位于中心位置，每个视点都有很高的概率捕捉到植物部位。然而，在评估方面，这种设置可能会缩小NBV和非NBV方法之间的差距，因为它使非NBV方法更容易选择具有高IG的视点。

# 4.3.2. 真实世界植物数量有限

本研究使用了三株真实植物来评估该方法在真实场景中的表现。虽然仅测试三株植物可能无法全面评估该方法的整体性能，因为植物变异的覆盖范围有限，但我们认为它仍然证明了该方法在真实植物重建中的可行性。首先，每株植物的数据都是从四个不同的位姿收集的，为训练和测试集增加了变化。其次，在模拟环境中进行了全面的评估，涵盖了广泛的植物变异，这有助于验证和支持真实世界的结果。第三，所有结果都是通过$\mathbf{k}$折交叉验证获得的，这增强了评估的稳健性。我们预计，在更多真实植物上测试时，这些方法会产生与我们的研究观察到的相似模式。然而，为了全面评估该方法在真实世界中的性能，我们建议进行进一步的实验，包括更广泛的植物和植物变异。

此外，尽管我们对点云进行了三步去噪处理，但在真实点云的边缘仍然存在显著的噪声(图14提供了可视化示例)。这种噪声降低了重建率，因为真实点云中存在的噪声可能在重建中缺失。为了最小化噪声的影响，我们将阈值$\delta$设置为$0.01\mathrm{m}$(详见公式(2)的解释)。虽然这个相对较大的阈值可能会通过轻松地将真实点云中的一个点计为重建点来高估重建性能，但它确保了比较的公平性，并且不会影响方法之间的比较，因为所有方法都应用了相同的阈值。我们相信，随着技术的进步和RGB-D相机变得更加精确和便宜，相机噪声的影响在未来会减少。此外，由于无法获得真实世界植物的完整3D模型，真实模型是通过合并所有视点的点云来近似的。虽然这种方法在真实模型中引入了缺陷，但它保持了评估的公平性，因为所有方法都使用相同的标准进行评估。这些缺陷对重建率的影响很难量化，因为它受到噪声、评估指标等多种因素的影响。然而，我们相信，即使有完美的真实模型，方法之间的比较性能仍将保持一致。

# 4.3.3. SSL-NBV训练迭代次数

在SSL-NBV在线训练期间，我们分别为实验S1、S2和RW设置了$T=50{,}000$、12,400和25,000次迭代。这些设置使该方法在显著减少训练时间的同时，实现了与PC-NBV相当的植物重建性能。然而，如图8和图13所示，随着训练迭代次数的增加，SSL-NBV的植物重建性能可以进一步提高，使其更接近PC-NBV。因此，我们认为，如果应用更长的训练迭代次数，图7、图10和图12中SSL-NBV的结果也可能显示出改进的性能，有可能达到与PC-NBV相似的结果。

此外，当SSL-NBV应用于已经学习过的植物或环境时，可以减少甚至消除自监督学习的迭代次数。在我们的实验中，自监督学习始终处于启用状态，以允许该方法适应新的植物和环境。虽然这确保了灵活性，但也引入了额外的网络优化时间。实验S2的结果表明，SSL-NBV对不同植物类型具有泛化能力，这表明对于熟悉的场景，可以停用自监督学习。对于已知的植物和环境，禁用此功能将显著提高NBV规划效率，并避免在不需要微调的植物上浪费时间，从而增强该方法的实用性。

# 5. 结论

本文提出了一种SSL-NBV算法，通过自监督学习增强基于学习的NBV方法，实现了神经网络的高效自动训练以预测下一个视点的信息增益（IG），从而优化3D植物重建。该方法利用机器人主动改变视点，实现了无需人工标注的终身持续在线学习。为了确保高效的在线学习，我们结合了改进的IG指标、经验回放和弱监督学习技术。通过k折交叉验证进行了全面评估，以回答研究问题。

实验S1的结果展示了不同多视图重建方法的重建质量和效率（回答研究问题1）。所提出的SSL-NBV方法优于非NBV方法，分别比预定义规划器和随机规划器少用1个和4个视点就达到了90%的植物重建水平。虽然SSL-NBV方法与其他NBV方法实现了相似的重建质量，但在计算速度上表现出色。具体而言，SSL-NBV在0.0038秒内完成IG预测，比经典的基于体素的NBV快842倍。当启用在线学习时，SSL-NBV需要额外的网络训练步骤，每次训练迭代耗时0.099秒，但仍比基于体素的NBV快32倍。与使用离线强监督学习的PC-NBV相比，所提出的方法仅使用9%的IG标注就实现了相当的植物重建性能（回答研究问题2）。此外，这是完全以自监督的方式实现的，无需人工标注或真实植物模型。在部署效率方面，对于小规模植物重建任务（少于422株植物），基于体素的NBV更为高效，因为它消除了数据整理和训练步骤。然而，随着植物数量的增加，SSL-NBV的效率优势变得越来越明显，使其更适合大规模应用。实验S2回答了研究问题3。经过在线自监督学习后，该方法分别比未微调的网络快1个和2个视点达到80%和90%的植物重建，展示了该方法通过在线自监督学习适应新环境的能力（回答研究问题3）。此外，在线学习期间植物重建的明显改进（如图8、图11和图13所示）进一步证实了该方法的适应性。最后，实验RW的结果表明，所提出的方法也可以成功应用于现实世界（回答研究问题4），在对真实植物进行在线微调后实现了92.5%的植物重建。它分别比随机规划器和预定义规划器高出约3.5%和8.5%，同时与PC-NBV相比减少了79%的IG标注需求。

总之，所提出的SSL-NBV方法能够在模拟和真实环境中高效地进行3D植物重建，并且可以通过在线自监督学习适应新环境，而无需任何人工干预。尽管这项工作是在单株植物上展示的，但我们相信该方法可以适应未来更复杂农业环境中的机器人植物重建研究。未来的改进应克服当前固定候选视点数量有限的局限性，转而采用更灵活的视点采样和视点轨迹规划。

# 作者贡献声明

Jianchao Ci：撰写初稿，软件开发，方法论，调查，形式分析，数据整理，概念化。Eldert J. van Henten：审阅和编辑，监督，资金获取，概念化。Xin Wang：审阅和编辑，监督，概念化。Akshay K. Burusa：审阅和编辑，软件开发，数据整理，概念化。Gert Kootstra：审阅和编辑，监督，资金获取，概念化。

# 利益冲突声明

作者声明，他们没有已知的可能影响本报告工作的竞争性财务利益或个人关系。

# 致谢

本研究得到了中国国家留学基金管理委员会（No.202107720034）和荷兰科学研究组织（NWO）项目"FlexCRAFT：灵活农业食品技术的认知机器人"（资助号P17-01）的支持。我们感谢瓦赫宁根大学农业生物系统工程（ABE）小组成员的深入讨论和宝贵反馈。

# 数据可用性声明

数据将根据请求提供。

# 参考文献

Atefi, A., Ge, Y., Pitla, S., Schnable, J., 2021. 高通量植物表型分析的机器人技术：当代综述与未来展望. Front Plant Sci. https://doi.org/10.3389/fpls.2021.611940.   
Atefi, A., Ge, Y., Pitla, S., Schnable, J., 2019. 温室中玉米和高粱叶片性状的类人机器人活体表型分析. Comput Electron Agric 163. https:// doi.org/10.1016/j.compag.2019.104854.   
Boogaard, F.P., van Henten, E.J., Kootstra, G., 2023. 3D点云在数字植物表型分析中的附加价值——以黄瓜节间长度测量为例. Biosyst Eng 234, 1–12. https://doi.org/10.1016/j. biosystemseng.2023.08.010.   
Boogaard, F.P., van Henten, E.J., Kootstra, G., 2022. 通过类别依赖的采样训练数据改进点云分割以应对类别不平衡问题. Front Plant Sci 13. https://doi.org/10.3389/fpls.2022.838190.   
Boogaard, F.P., van Henten, E.J., Kootstra, G., 2021. 通过光谱数据增强不完整3D点云来提升黄瓜植株部位分割. Biosyst Eng 211, 167–182. https://doi.org/10.1016/j.biosystemseng.2021.09.004.   
Burusa, A.K., van Henten, E.J., Kootstra, G., 2022. 基于注意力机制的主动视觉用于植物及目标部位的高效重建.   
Cheng, Y., Lan, S., Fan, X., Tjahjadi, T., Jin, S., Cao, L., 2023. 基于双分支弱监督学习的网络用于从遥感图像中精确绘制木本植被. Int J Appl Earth Obs Geoinf 124, 103499.   
Deng, X., Xiang, Y., Mousavian, A., Eppner, C., Bretl, T., Fox, D., 2020. 用于机器人操作的6D物体姿态自监督估计. Proc - IEEE Int. Conf. Robot Automat. 3665–3671. https://doi.org/10.1109/ICRA40945.2020.9196714.   
Gibbs, J.A., Pound, M., French, A.P., Wells, D.M., Murchie, E., Pridmore, T., 2018. 植物表型分析：用于三维植物茎重建的主动视觉单元[OPEN]. Plant Physiol. 178, 524–534. https://doi.org/10.1104/PP.18.00664.   
Gibbs, J.A., Pound, M.P., French, A.P., Wells, D.M., Murchie, E.H., Pridmore, T.P., 2020. 用于3D植物茎建模的主动视觉与表面重建. IEEE/ACM Trans Comput Biol Bioinf 17, 1907–1917. https://doi.org/10.1109/ TCBB.2019.2896908.   
Golbach, F., Kootstra, G., Damjanovic, S., Otten, G., van de Zedde, R., 2016. 使用适合高通量幼苗表型分析的3D重建方法验证植物部位测量. Mach Vis Appl 27, 663–680. https://doi.org/ 10.1007/s00138-015-0727-5.   
Han, Y., Zhan, I.H., Zhao, W., Liu, Y.-J., 2022. 用于主动物体重建的双分支最佳视角网络及新型机器人系统.   
Hartmann, A., Czauderna, T., Hoffmann, R., Stein, N., Schreiber, F., 2011. HTPheno：高通量植物表型分析的图像处理流程. BMC Bioinf 12. https://doi.org/10.1186/1471-2105-12-148.   
Jansen, M., Gilmer, F., Biskup, B., Nagel, K.A., Rascher, U., Fischbach, A., Briem, S., Dreissen, G., Tittmann, S., Braun, S., De Jaeger, I., Metzlaff, M., Schurr, U., Scharr, H., Walter, A., 2009. 通过Growscreen Fluoro同时进行叶片生长和叶绿素荧光表型分析，可检测拟南芥和其他莲座植物的胁迫耐受性. Funct Plant Biol 36, 902–914. https://doi.org/10.1071/FP09095.   
Kahn, G., Abbeel, P., Levine, S., 2020. BADGR：基于自监督学习的自主导航系统.   
Koenig, N., Howard, A., 2004. Gazebo开源多机器人模拟器的设计和使用范式. In: 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No. 04CH37566). IEEE, pp. 2149–2154.   
Lehnert, C., Tsai, D., Eriksson, A., McCool, C., 2018. 3D Move to See：通过语义分割改进物体视图的多视角视觉伺服.   
Lu, H., Tang, L., Whitham, S.A., Mei, Y., 2017. 用于玉米幼苗形态特征表征的机器人平台. Sensors (Switzerland) 17. https://doi.org/ 10.3390/s17092082.   
Mendoza, M., Vasquez-Gomez, J.I., Taud, H., Sucar, L.E., Reta, C., 2020. 用于3D物体重建的最佳视角监督学习. Pattern Recogn Lett 133, 224–231. https://doi.org/10.1016/j.patrec.2020.02.024.   
Minervini, M., Abdelsamea, M.M., Tsaftaris, S.A., 2014. 基于增量学习和主动轮廓的图像植物表型分析. Ecol Inform 23, 35–48. https://doi. org/10.1016/j.ecoinf.2013.07.004.   
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., 2013. 使用深度强化学习玩Atari游戏.   
Poland, J.A., Rife, T.W., 2012. 用于植物育种和遗传学的测序基因分型. Plant Genome 5. https://doi.org/10.3835/plantgenome2012.05.0005.   
Polder, G., Hofstee, J.W., 2014. 使用3D光场相机对温室中的大型番茄植株进行表型分析. In: 2014 Montreal, Quebec Canada July 13–July 16, 2014. American Society of Agricultural and Biological Engineers, p. 1.   
Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R., Ng, A.Y., 2009. ROS：一个开源的机器人操作系统。In: ICRA Workshop on Open Source Software. Kobe, Japan, p. 5.   
Shi, W., van de Zedde, R., Jiang, H., Kootstra, G., 2019. 使用深度学习和多视角视觉进行植物部位分割。Biosyst Eng 187, 81–95. https://doi.org/ 10.1016/j.biosystemseng.2019.08.014.   
Stutz, D., Geiger, A., 2020. 弱监督下的3D形状补全学习。Int J Comput Vis 128, 1162–1181. https://doi.org/10.1007/s11263-018-1126-y.   
Thapa, S., Zhu, F., Walia, H., Yu, H., Ge, Y., 2018. 一种基于LiDAR的新型仪器，用于玉米和高粱形态特征的高通量3D测量。Sensors (switzerland) 18. https://doi.org/10.3390/s18041187.   
Tisne´, S., Serrand, Y., Bach, L., Gilbault, E., Ben Ameur, R., Balasse, H., Voisin, R., Bouchez, D., Durand-Tardif, M., Guerche, P., Chareyron, G., Da Rugna, J., Camilleri, C., Loudet, O., 2013. Phenoscope：一个提供高空间均匀性的自动化大规模表型分析平台。Plant J 74, 534–544. https://doi.org/ 10.1111/tpj.12131.   
van der Heijden, G., Song, Y., Horgan, G., Polder, G., Dieleman, A., Bink, M., Palloix, A., van Eeuwijk, F., Glasbey, C., 2012. SPICY：面向温室中大型辣椒植株的自动化表型分析。Funct Plant Biol 39, 870–877.   
Va´zquez-Arellano, M., Reiser, D., Paraforos, D.S., Garrido-Izard, M., Griepentrog, H.W., 2018. 使用基于不同扫描方向的飞行时间相机估算重建玉米植株的叶面积。Robotics 7. https://doi.org/10.3390/ robotics7040063.   
Wang, Y., James, S., Stathopoulou, E.K., Beltra´n-Gonza´lez, C., Konishi, Y., Del Bue, A., 2019. 使用机械臂进行室内环境的自主3D重建、映射和探索。IEEE Robot Autom Lett 4, 3340–3347.   
Wang, Z., Schaul, T., Hessel, M., Lanctot, M., 2016. 深度强化学习的双网络架构 Hado van Hasselt.   
Zaenker, T., Lehnert, C., McCool, C., Bennewitz, M., 2021a. 结合局部和全局视点规划以实现水果覆盖。In: 2021 10th European Conference on Mobile Robots, ECMR 2021 - Proceedings. 10.1109/ECMR50962.2021.9568836.   
Zaenker, T., Smitt, C., McCool, C., Bennewitz, M., 2021b. 用于水果大小和位置估计的视点规划。IEEE Int. Conf. Intell. Robot. Syst. 3271–3277. https:// doi.org/10.1109/IROS51168.2021.9636701.   
Zapotezny-Anderson, P., Lehnert, C., 2019. 迈向农业中的主动机器人视觉：在遮挡和非结构化保护种植环境中进行视觉伺服控制的深度学习方法。IFAC-PapersOnLine. 120–125. https://doi.org/10.1016/j. ifacol.2019.12.508.   
Zeng, R., Zhao, W., Liu, Y.J., 2020. PC-NBV：一种基于点云的高效最佳视点规划深度网络，in: IEEE International Conference on Intelligent Robots and Systems. Institute of Electrical and Electronics Engineers Inc., pp. 7050–7057. 10.1109/IROS45743.2020.9340916.   
Zeng, X., Zaenker, T., Bennewitz, M., 2022. 深度强化学习在农业应用中的最佳视点规划 2323–2329.   
Zhang, H., Goodfellow, I., Metaxas, D., Odena, A., 2019. 自注意力生成对抗网络。Int Conf Mach Learn. PMLR 7354–7363.   
Zhang, S., Sutton, R.S., 2017. 深入探讨经验回放。   
Zhou, Z.H., 2018. 弱监督学习简介。Natl Sci Rev. https://doi.org/10.1093/nsr/nwx106.  

# References  

Atefi, A., Ge, Y., Pitla, S., Schnable, J., 2021. Robotic technologies for high-throughput plant phenotyping: contemporary reviews and future perspectives. Front Plant Sci. https://doi.org/10.3389/fpls.2021.611940.   
Atefi, A., Ge, Y., Pitla, S., Schnable, J., 2019. In vivo human-like robotic phenotyping of leaf traits in maize and sorghum in greenhouse. Comput Electron Agric 163. https:// doi.org/10.1016/j.compag.2019.104854.   
Boogaard, F.P., van Henten, E.J., Kootstra, G., 2023. The added value of 3D point clouds for digital plant phenotyping – a case study on internode length measurements in cucumber. Biosyst Eng 234, 1–12. https://doi.org/10.1016/j. biosystemseng.2023.08.010.   
Boogaard, F.P., van Henten, E.J., Kootstra, G., 2022. Improved point-cloud segmentation for plant phenotyping through class-dependent sampling of training data to battle class imbalance. Front Plant Sci 13. https://doi.org/10.3389/fpls.2022.838190.   
Boogaard, F.P., van Henten, E.J., Kootstra, G., 2021. Boosting plant-part segmentation of cucumber plants by enriching incomplete 3D point clouds with spectral data. Biosyst Eng 211, 167–182. https://doi.org/10.1016/j.biosystemseng.2021.09.004.   
Burusa, A.K., van Henten, E.J., Kootstra, G., 2022. Attention-driven Active Vision for Efficient Reconstruction of Plants and Targeted Plant Parts.   
Cheng, Y., Lan, S., Fan, X., Tjahjadi, T., Jin, S., Cao, L., 2023. A dual-branch weakly supervised learning based network for accurate mapping of woody vegetation from remote sensing images. Int J Appl Earth Obs Geoinf 124, 103499.   
Deng, X., Xiang, Y., Mousavian, A., Eppner, C., Bretl, T., Fox, D., 2020. Self-supervised 6D Object pose estimation for robot manipulation. Proc - IEEE Int. Conf. Robot Automat. 3665–3671. https://doi.org/10.1109/ICRA40945.2020.9196714.   
Gibbs, J.A., Pound, M., French, A.P., Wells, D.M., Murchie, E., Pridmore, T., 2018. Plant phenotyping: an active vision cell for three-dimensional plant shoot reconstruction1 [OPEN]. Plant Physiol. 178, 524–534. https://doi.org/10.1104/PP.18.00664.   
Gibbs, J.A., Pound, M.P., French, A.P., Wells, D.M., Murchie, E.H., Pridmore, T.P., 2020. Active vision and surface reconstruction for 3D plant shoot modelling. IEEE/ACM Trans Comput Biol Bioinf 17, 1907–1917. https://doi.org/10.1109/ TCBB.2019.2896908.   
Golbach, F., Kootstra, G., Damjanovic, S., Otten, G., van de Zedde, R., 2016. Validation of plant part measurements using a 3D reconstruction method suitable for highthroughput seedling phenotyping. Mach Vis Appl 27, 663–680. https://doi.org/ 10.1007/s00138-015-0727-5.   
Han, Y., Zhan, I.H., Zhao, W., Liu, Y.-J., 2022. A double branch next-best-view network and novel robot system for active object reconstruction.   
Hartmann, A., Czauderna, T., Hoffmann, R., Stein, N., Schreiber, F., 2011. HTPheno: An image analysis pipeline for high-throughput plant phenotyping. BMC Bioinf 12. https://doi.org/10.1186/1471-2105-12-148.   
Jansen, M., Gilmer, F., Biskup, B., Nagel, K.A., Rascher, U., Fischbach, A., Briem, S., Dreissen, G., Tittmann, S., Braun, S., De Jaeger, I., Metzlaff, M., Schurr, U., Scharr, H., Walter, A., 2009. Simultaneous phenotyping of leaf growth and chlorophyll fluorescence via Growscreen Fluoro allows detection of stress tolerance in Arabidopsis thaliana and other rosette plants. Funct Plant Biol 36, 902–914. https://doi.org/10.1071/FP09095.   
Kahn, G., Abbeel, P., Levine, S., 2020. BADGR: An Autonomous Self-Supervised Learning-Based Navigation System.   
Koenig, N., Howard, A., 2004. Design and use paradigms for gazebo, an open-source multi-robot simulator. In: 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No. 04CH37566). IEEE, pp. 2149–2154.   
Lehnert, C., Tsai, D., Eriksson, A., McCool, C., 2018. 3D Move to See: Multi-perspective visual servoing for improving object views with semantic segmentation.   
Lu, H., Tang, L., Whitham, S.A., Mei, Y., 2017. A robotic platform for corn seedling morphological traits characterization. Sensors (Switzerland) 17. https://doi.org/ 10.3390/s17092082.   
Mendoza, M., Vasquez-Gomez, J.I., Taud, H., Sucar, L.E., Reta, C., 2020. Supervised learning of the next-best-view for 3d object reconstruction. Pattern Recogn Lett 133, 224–231. https://doi.org/10.1016/j.patrec.2020.02.024.   
Minervini, M., Abdelsamea, M.M., Tsaftaris, S.A., 2014. Image-based plant phenotyping with incremental learning and active contours. Ecol Inform 23, 35–48. https://doi. org/10.1016/j.ecoinf.2013.07.004.   
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., 2013. Playing Atari with Deep Reinforcement Learning.   
Poland, J.A., Rife, T.W., 2012. Genotyping-by-sequencing for plant breeding and genetics. Plant Genome 5. https://doi.org/10.3835/plantgenome2012.05.0005.   
Polder, G., Hofstee, J.W., 2014. Phenotyping large tomato plants in the greenhouse using a 3D light-field camera. In: 2014 Montreal, Quebec Canada July 13–July 16, 2014. American Society of Agricultural and Biological Engineers, p. 1.   
Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R., Ng, A.Y., 2009. ROS: an open-source Robot Operating System. In: ICRA Workshop on Open Source Software. Kobe, Japan, p. 5.   
Shi, W., van de Zedde, R., Jiang, H., Kootstra, G., 2019. Plant-part segmentation using deep learning and multi-view vision. Biosyst Eng 187, 81–95. https://doi.org/ 10.1016/j.biosystemseng.2019.08.014.   
Stutz, D., Geiger, A., 2020. Learning 3D shape completion under weak supervision. Int J Comput Vis 128, 1162–1181. https://doi.org/10.1007/s11263-018-1126-y.   
Thapa, S., Zhu, F., Walia, H., Yu, H., Ge, Y., 2018. A novel LiDAR-Based instrument for high-throughput, 3D measurement of morphological traits in maize and sorghum. Sensors (switzerland) 18. https://doi.org/10.3390/s18041187.   
Tisne´, S., Serrand, Y., Bach, L., Gilbault, E., Ben Ameur, R., Balasse, H., Voisin, R., Bouchez, D., Durand-Tardif, M., Guerche, P., Chareyron, G., Da Rugna, J., Camilleri, C., Loudet, O., 2013. Phenoscope: an automated large-scale phenotyping platform offering high spatial homogeneity. Plant J 74, 534–544. https://doi.org/ 10.1111/tpj.12131.   
van der Heijden, G., Song, Y., Horgan, G., Polder, G., Dieleman, A., Bink, M., Palloix, A., van Eeuwijk, F., Glasbey, C., 2012. SPICY: towards automated phenotyping of large pepper plants in the greenhouse. Funct Plant Biol 39, 870–877.   
Va´zquez-Arellano, M., Reiser, D., Paraforos, D.S., Garrido-Izard, M., Griepentrog, H.W., 2018. Leaf area estimation of reconstructed maize plants using a time-of-flight camera based on different scan directions. Robotics 7. https://doi.org/10.3390/ robotics7040063.   
Wang, Y., James, S., Stathopoulou, E.K., Beltra´n-Gonza´lez, C., Konishi, Y., Del Bue, A., 2019. Autonomous 3-d reconstruction, mapping, and exploration of indoor environments with a robotic arm. IEEE Robot Autom Lett 4, 3340–3347.   
Wang, Z., Schaul, T., Hessel, M., Lanctot, M., 2016. Dueling Network Architectures for Deep Reinforcement Learning Hado van Hasselt.   
Zaenker, T., Lehnert, C., McCool, C., Bennewitz, M., 2021a. Combining local and global viewpoint planning for fruit coverage. In: 2021 10th European Conference on Mobile Robots, ECMR 2021 - Proceedings. 10.1109/ECMR50962.2021.9568836.   
Zaenker, T., Smitt, C., McCool, C., Bennewitz, M., 2021b. Viewpoint planning for fruit size and position estimation. IEEE Int. Conf. Intell. Robot. Syst. 3271–3277. https:// doi.org/10.1109/IROS51168.2021.9636701.   
Zapotezny-Anderson, P., Lehnert, C., 2019. Towards active robotic vision in agriculture: a deep learning approach to visual servoing in occluded and unstructured protected cropping environments. IFAC-PapersOnLine. 120–125. https://doi.org/10.1016/j. ifacol.2019.12.508.   
Zeng, R., Zhao, W., Liu, Y.J., 2020. PC-NBV: A point cloud based deep network for efficient next best view planning, in: IEEE International Conference on Intelligent Robots and Systems. Institute of Electrical and Electronics Engineers Inc., pp. 7050–7057. 10.1109/IROS45743.2020.9340916.   
Zeng, X., Zaenker, T., Bennewitz, M., 2022. Deep Reinforcement Learning for Next-BestView Planning in Agricultural Applications 2323–2329.   
Zhang, H., Goodfellow, I., Metaxas, D., Odena, A., 2019. Self-attention generative adversarial networks. Int Conf Mach Learn. PMLR 7354–7363.   
Zhang, S., Sutton, R.S., 2017. A Deeper Look at Experience Replay.   
Zhou, Z.H., 2018. A brief introduction to weakly supervised learning. Natl Sci Rev. https://doi.org/10.1093/nsr/nwx106.  